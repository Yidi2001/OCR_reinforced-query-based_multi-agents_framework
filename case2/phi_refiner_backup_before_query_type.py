#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phi3.5-Vision Refiner
åŸºäºOCRç»“æœï¼Œè®©Phi3.5é‡æ–°ç†è§£å’Œå›ç­”ç”¨æˆ·query
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any, Optional
import torch
from PIL import Image
from transformers import AutoModelForCausalLM, AutoProcessor

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
sys.path.insert(0, str(Path(__file__).parent))


class PhiRefiner:
    """
    Phi3.5-Vision Refiner
    åˆ©ç”¨OCRè¯†åˆ«ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè®©Phi3.5åŸºäºç”¨æˆ·queryç»™å‡ºæ›´å‡†ç¡®çš„å›ç­”
    """
    
    def __init__(self, model_path: str = "models/phi-3_5_vision", ctx: Optional['RuntimeContext'] = None):
        """
        Args:
            model_path: Phi3.5-Visionæ¨¡å‹è·¯å¾„
            ctx: RuntimeContext å®ä¾‹ï¼ˆç”¨äºå…±äº«æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        """
        self.model_path = model_path
        self.ctx = ctx
        self.model = None
        self.processor = None
        self.device = None
        self._model_loaded = False
    
    def load_model(self):
        """åŠ è½½Phi3.5-Visionæ¨¡å‹"""
        if self._model_loaded:
            return
        
        print(f"[PhiRefiner] åˆå§‹åŒ–: {self.model_path}")
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ä» RuntimeContext è·å–æˆ–åˆ›å»ºæ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        if self.ctx is not None:
            self.model = self._get_model_from_ctx()
            self.processor = self._get_processor_from_ctx()
        else:
            # é€€åŒ–æ¨¡å¼ï¼šä¸ä½¿ç”¨ ctxï¼ˆå‘åå…¼å®¹ï¼‰
            print(f"  âš ï¸  æœªæä¾› RuntimeContextï¼Œå°†ç‹¬ç«‹åŠ è½½æ¨¡å‹")
            self.model = self._load_model()
            self.processor = self._load_processor()
        
        self._model_loaded = True
        print(f"  âœ“ PhiRefiner å‡†å¤‡å®Œæˆ (device: {self.device})")
    
    def _get_model_from_ctx(self):
        """ä» RuntimeContext è·å– Phi æ¨¡å‹"""
        from runtime_context import make_model_key
        key = make_model_key("phi_model", self.model_path)
        return self.ctx.get(key, self._load_model)
    
    def _get_processor_from_ctx(self):
        """ä» RuntimeContext è·å– Phi processor"""
        from runtime_context import make_model_key
        key = make_model_key("phi_processor", self.model_path)
        return self.ctx.get(key, self._load_processor)
    
    def _load_model(self):
        """åŠ è½½ Phi æ¨¡å‹ï¼ˆå·¥å‚å‡½æ•°ï¼‰"""
        return AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map="cuda" if torch.cuda.is_available() else "cpu",
            trust_remote_code=True,
            torch_dtype="auto",
            _attn_implementation='eager'
        )
        
    def _load_processor(self):
        """åŠ è½½ Phi processorï¼ˆå·¥å‚å‡½æ•°ï¼‰"""
        return AutoProcessor.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            num_crops=4
        )
    
    def refine_with_ocr_context(self, 
                                 image_path: str, 
                                 user_query: str,
                                 ocr_summary_text: str) -> str:
        """
        åŸºäºOCRç»“æœæç¤ºï¼Œè®©Phi3.5é‡æ–°è¿›è¡ŒOCRè¯†åˆ«
        
        Args:
            image_path: åŸå§‹å›¾ç‰‡è·¯å¾„
            user_query: ç”¨æˆ·çš„æŸ¥è¯¢/ä»»åŠ¡
            ocr_summary_text: OCRè¯†åˆ«ç»“æœçš„æ‘˜è¦æ–‡æœ¬
            
        Returns:
            Phi3.5åŸºäºOCRæç¤ºå’Œå›¾ç‰‡ç»™å‡ºçš„è¯†åˆ«ç»“æœ
        """
        self.load_model()
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_refine_prompt(user_query, ocr_summary_text)
        
        # åŠ è½½å›¾ç‰‡
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            return f"é”™è¯¯ï¼šæ— æ³•åŠ è½½å›¾ç‰‡ - {e}"
        
        # å‡†å¤‡è¾“å…¥
        messages = [
            {"role": "user", "content": f"<|image_1|>\n{prompt}"}
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        prompt_text = self.processor.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            prompt_text,
            [image],
            return_tensors="pt"
        ).to(self.device)
        
        # ç”Ÿæˆè¾“å‡º
        print(f"[PhiRefiner] æ­£åœ¨åŸºäºOCRç»“æœåˆ†æå›¾ç‰‡...")
        
        generation_args = {
            "max_new_tokens": 2000,
            "temperature": 0.1,
            "do_sample": False,
            "use_cache": False,  # é¿å…cacheç›¸å…³çš„å…¼å®¹æ€§é—®é¢˜
        }
        
        with torch.no_grad():
            generate_ids = self.model.generate(
                **inputs,
                eos_token_id=self.processor.tokenizer.eos_token_id,
                **generation_args
            )
        
        # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å†…å®¹
        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]
        response = self.processor.batch_decode(
            generate_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
        response_stripped = response.strip()
        if not response_stripped:
            print(f"[PhiRefiner] âš ï¸  è­¦å‘Š: æ¨¡å‹ç”Ÿæˆäº†ç©ºå“åº”")
            print(f"[PhiRefiner]   åŸå§‹å“åº”é•¿åº¦: {len(response)}")
            print(f"[PhiRefiner]   ç”Ÿæˆçš„tokenæ•°: {generate_ids.shape[1]}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤æ¶ˆæ¯è€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²
            return "[æ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå“åº”]"
        
        return response_stripped
    
    def _build_refine_prompt(self, user_query: str, ocr_summary_text: str) -> str:
        """
        æ„å»ºç»™Phi3.5çš„æç¤ºè¯ - è®©å®ƒåŸºäºå›¾ç‰‡å’ŒOCRç»“æœå›ç­”ç”¨æˆ·é—®é¢˜
        """
        prompt = f"""You are a visual question answering assistant. You must LOOK AT THE IMAGE and use the OCR reference below to help you answer accurately.

Question: {user_query}

OCR Reference (sorted by relevance to the question):
**IMPORTANT**: The layout regions below are ranked by relevance. The FIRST regions are MOST RELEVANT.

{ocr_summary_text}

Instructions:
1. LOOK AT THE IMAGE carefully - the OCR text is just a reference
2. Use the OCR text to locate relevant regions in the image
3. Verify the answer by examining the actual image content
4. Focus on top-ranked regions first (they are most relevant to the question)
5. Give ONLY the direct answer - be as brief as possible (1-2 sentences max)
6. The answers should not be repetitive and should not contain any repetitive content
7. If asking for a name/number/entity, output only that information
8. Do NOT explain, do NOT repeat the question

Answer:"""
        
        return prompt
    
    def refine_from_summary_file(self, summary_json_path: str) -> Dict[str, Any]:
        """
        ä»æ‘˜è¦JSONæ–‡ä»¶è¯»å–ä¿¡æ¯ï¼Œå¹¶è°ƒç”¨Phi3.5è¿›è¡Œrefinement
        
        Args:
            summary_json_path: æ‘˜è¦JSONæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
                - æ ¼å¼1: *_summary.json + å¯¹åº”çš„ *_prompt.txt
                - æ ¼å¼2: åŒ…å« blocks æ•°ç»„çš„ JSONï¼ˆå¦‚ evidence.jsonï¼‰
            
        Returns:
            åŒ…å«refinementç»“æœçš„å­—å…¸
        """
        # è¯»å–æ‘˜è¦
        with open(summary_json_path, 'r', encoding='utf-8') as f:
            summary = json.load(f)
        
        image_path = summary.get('image_path', '')
        user_query = summary.get('user_query', summary.get('query', 'è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—'))
        
        # å°è¯•æ–¹æ³•1ï¼šæŸ¥æ‰¾å¯¹åº”çš„ prompt.txt æ–‡ä»¶
        summary_path = Path(summary_json_path)
        
        # å°è¯•å¤šç§å‘½åæ–¹å¼
        possible_prompt_paths = [
            # æ–¹å¼1: åœ¨åŒä¸€æ–‡ä»¶å¤¹ä¸­çš„ prompt.txtï¼ˆæ ·æœ¬æ–‡ä»¶å¤¹æ¨¡å¼ï¼‰
            summary_path.parent / 'prompt.txt',
            # æ–¹å¼2: xxx_summary.json -> xxx_prompt.txtï¼ˆæ—§æ¨¡å¼ï¼‰
            summary_path.parent / summary_path.name.replace('_summary.json', '_prompt.txt').replace('.json', '.txt')
        ]
        
        ocr_summary_text = None
        source_info = None
        
        # å°è¯•æ‰¾åˆ° prompt æ–‡ä»¶
        for prompt_txt_path in possible_prompt_paths:
            if prompt_txt_path.exists() and str(prompt_txt_path) != str(summary_path):
                with open(prompt_txt_path, 'r', encoding='utf-8') as f:
                    ocr_summary_text = f.read()
                source_info = prompt_txt_path.name
                break
        
        # æ–¹æ³•2ï¼šä» JSON çš„ blocks ä¸­æå–æ–‡æœ¬
        if not ocr_summary_text and 'blocks' in summary:
            ocr_summary_text = self._extract_text_from_blocks(summary['blocks'])
            source_info = f"ä» {summary_path.name} çš„ blocks æå–"
        
        # æ–¹æ³•3ï¼šä» JSON çš„ ocr_results ä¸­æå–æ–‡æœ¬
        if not ocr_summary_text and 'ocr_results' in summary:
            ocr_results = summary['ocr_results']
            if isinstance(ocr_results, dict):
                # æ ¼å¼1: {"type": "whole_image", "text": "..."}
                if 'text' in ocr_results:
                    ocr_summary_text = ocr_results['text']
                    source_info = f"ä» {summary_path.name} çš„ ocr_results.text æå–"
                # æ ¼å¼2: {"total_regions": N, "blocks": [...]}
                elif 'blocks' in ocr_results:
                    ocr_summary_text = self._extract_text_from_blocks(ocr_results['blocks'])
                    source_info = f"ä» {summary_path.name} çš„ ocr_results.blocks æå–"
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        if not ocr_summary_text:
            return {
                "error": f"æ— æ³•è·å–OCRæ–‡æœ¬ã€‚æœªæ‰¾åˆ°å¯¹åº”çš„ prompt.txtï¼ŒJSONä¸­ä¹Ÿæ²¡æœ‰ blocks æˆ– ocr_results.text å­—æ®µã€‚"
            }
        
        # æ£€æŸ¥ OCR æ–‡æœ¬è´¨é‡
        if len(ocr_summary_text.strip()) < 10:
            print(f"[PhiRefiner] âš ï¸  è­¦å‘Š: OCRæ–‡æœ¬è¿‡çŸ­ (é•¿åº¦: {len(ocr_summary_text)})")
            print(f"[PhiRefiner]   OCRæ–‡æœ¬: '{ocr_summary_text}'")
        
        # è°ƒç”¨Phi3.5
        refined_response = self.refine_with_ocr_context(
            image_path=image_path,
            user_query=user_query,
            ocr_summary_text=ocr_summary_text
        )
        
        return {
            "image_path": image_path,
            "user_query": user_query,
            "refined_response": refined_response,
            "ocr_summary_used": source_info
        }
    
    def _extract_text_from_blocks(self, blocks: list) -> str:
        """
        ä» blocks æ•°ç»„ä¸­æå– OCR æ–‡æœ¬å¹¶æ ¼å¼åŒ–
        
        Args:
            blocks: åŒ…å« region_id, text, label, bbox ç­‰ä¿¡æ¯çš„åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ OCR æ‘˜è¦æ–‡æœ¬
        """
        if not blocks:
            return ""
        
        lines = ["ã€OCRè¯†åˆ«ç»“æœã€‘\n"]
        
        for i, block in enumerate(blocks, 1):
            region_id = block.get('region_id', i)
            label = block.get('label', 'æœªçŸ¥ç±»å‹')
            text = block.get('text', '').strip()
            confidence = block.get('confidence', 0.0)
            bbox = block.get('bbox', [])
            
            lines.append(f"åŒºåŸŸ {region_id} ({label}, ç½®ä¿¡åº¦: {confidence:.2f}):")
            if text:
                lines.append(f"{text}")
            else:
                lines.append("(æ— æ–‡æœ¬)")
            lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(lines)


def test_refiner():
    """æµ‹è¯•Phi Refiner"""
    print("=" * 80)
    print("æµ‹è¯• Phi3.5-Vision Refiner")
    print("=" * 80)
    
    # æµ‹è¯•æ–‡ä»¶
    summary_file = "evidence.json"
    
    if not Path(summary_file).exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {summary_file}")
        return
    
    # åˆ›å»ºRefiner
    refiner = PhiRefiner()
    
    # æ‰§è¡Œrefinement
    print("\næ­£åœ¨åŸºäºOCRç»“æœè°ƒç”¨Phi3.5...")
    result = refiner.refine_from_summary_file(summary_file)
    
    if "error" in result:
        print(f"âŒ é”™è¯¯: {result['error']}")
        return
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 80)
    print("Phi3.5 Refinement ç»“æœ")
    print("=" * 80)
    print(f"\nğŸ“· å›¾åƒ: {result['image_path']}")
    print(f"â“ ç”¨æˆ·æŸ¥è¯¢: {result['user_query']}")
    print(f"ğŸ“„ ä½¿ç”¨çš„OCRæ‘˜è¦: {result['ocr_summary_used']}")
    print("\n" + "-" * 80)
    print("ğŸ’¡ Phi3.5 çš„å›ç­”:")
    print("-" * 80)
    print(result['refined_response'])
    print("\n" + "=" * 80)
    
    # ä¿å­˜ç»“æœ
    output_file = "evidence_refined.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    test_refiner()

