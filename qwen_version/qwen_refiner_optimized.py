#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen2-VL Refiner (Optimized Version)
åŸºäºOCRç»“æœï¼Œè®©Qwen2-VLé‡æ–°ç†è§£å’Œå›ç­”ç”¨æˆ·query
ä¼˜åŒ–ç‰ˆï¼šå‡å°‘OCRå™ªå£°å¹²æ‰°ï¼Œå¢å¼ºè§†è§‰éªŒè¯ï¼Œæå‡å‡†ç¡®ç‡
"""

import sys
import json
import re
from pathlib import Path
from typing import Dict, Any, Optional, List
import torch
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
sys.path.insert(0, str(Path(__file__).parent))


class QwenRefinerOptimized:
    """
    Qwen2-VL Refiner (Optimized Version)
    åˆ©ç”¨OCRè¯†åˆ«ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè®©Qwen2-VLåŸºäºç”¨æˆ·queryç»™å‡ºæ›´å‡†ç¡®çš„å›ç­”
    
    ä¼˜åŒ–ç‚¹ï¼š
    1. OCRå™ªå£°è¿‡æ»¤ï¼šåªä¿ç•™top-3æœ€ç›¸å…³åŒºåŸŸ
    2. Queryç±»å‹è‡ªé€‚åº”ï¼šé’ˆå¯¹number/title/nameç­‰ç”Ÿæˆç‰¹å®šæç¤º
    3. å¼ºåŒ–æŒ‡ä»¤ï¼šSTEP-BY-STEP + CRITICAL RULES
    4. å¼ºè°ƒè§†è§‰éªŒè¯ï¼šä¼˜å…ˆçœ‹å›¾ï¼ŒOCRä»…ä½œä½ç½®æç¤º
    """
    
    def __init__(self, model_path: str = "models/Qwen2-VL-2B-Instruct", ctx: Optional['RuntimeContext'] = None):
        """
        Args:
            model_path: Qwen2-VLæ¨¡å‹è·¯å¾„
            ctx: RuntimeContext å®ä¾‹ï¼ˆç”¨äºå…±äº«æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        """
        self.model_path = model_path
        self.ctx = ctx
        self.model = None
        self.processor = None
        self.device = None
        self._model_loaded = False
    
    def load_model(self):
        """åŠ è½½Qwen2-VLæ¨¡å‹"""
        if self._model_loaded:
            return
        
        print(f"[QwenRefinerOptimized] åˆå§‹åŒ–: {self.model_path}")
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ä» RuntimeContext è·å–æˆ–åˆ›å»ºæ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        if self.ctx is not None:
            self.model = self._get_model_from_ctx()
            self.processor = self._get_processor_from_ctx()
        else:
            # é€€åŒ–æ¨¡å¼ï¼šä¸ä½¿ç”¨ ctxï¼ˆå‘åå…¼å®¹ï¼‰
            print(f"  âš ï¸  æœªæä¾› RuntimeContextï¼Œå°†ç‹¬ç«‹åŠ è½½æ¨¡å‹")
            self.model = self._load_model()
            self.processor = self._load_processor()
        
        self._model_loaded = True
        print(f"  âœ“ QwenRefinerOptimized å‡†å¤‡å®Œæˆ (device: {self.device})")
    
    def _get_model_from_ctx(self):
        """ä» RuntimeContext è·å– Qwen2 æ¨¡å‹"""
        from runtime_context import make_model_key
        key = make_model_key("qwen2_model", self.model_path)
        return self.ctx.get(key, self._load_model)
    
    def _get_processor_from_ctx(self):
        """ä» RuntimeContext è·å– Qwen2 processor"""
        from runtime_context import make_model_key
        key = make_model_key("qwen2_processor", self.model_path)
        return self.ctx.get(key, self._load_processor)
    
    def _load_model(self):
        """åŠ è½½ Qwen2 æ¨¡å‹ï¼ˆå·¥å‚å‡½æ•°ï¼‰"""
        return Qwen2VLForConditionalGeneration.from_pretrained(
            self.model_path,
            torch_dtype="auto",
            device_map="auto"
        )
        
    def _load_processor(self):
        """åŠ è½½ Qwen2 processorï¼ˆå·¥å‚å‡½æ•°ï¼‰"""
        return AutoProcessor.from_pretrained(self.model_path)
    
    def _clean_ocr_context(self, ocr_summary_text: str, top_k: int = 3) -> str:
        """
        æ¸…æ´—OCRä¸Šä¸‹æ–‡ï¼Œåªä¿ç•™æœ€ç›¸å…³çš„top-kä¸ªåŒºåŸŸï¼Œå‡å°‘å™ªå£°å¹²æ‰°
        
        Args:
            ocr_summary_text: åŸå§‹OCRæ‘˜è¦æ–‡æœ¬
            top_k: ä¿ç•™çš„åŒºåŸŸæ•°é‡ï¼ˆé»˜è®¤3ï¼‰
            
        Returns:
            æ¸…æ´—åçš„OCRæ–‡æœ¬
        """
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼ˆ<200å­—ç¬¦ï¼‰ï¼Œç›´æ¥è¿”å›
        if len(ocr_summary_text) < 200:
            return ocr_summary_text
        
        # è§£æåŒºåŸŸä¿¡æ¯ï¼ˆå‡è®¾æ ¼å¼ä¸º "Region X: ..." æˆ– "åŒºåŸŸXï¼š..."ï¼‰
        region_pattern = r'(?:Region|åŒºåŸŸ)\s*\d+[:ï¼š]\s*([^\n]+)'
        regions = re.findall(region_pattern, ocr_summary_text, re.IGNORECASE)
        
        if len(regions) <= top_k:
            # åŒºåŸŸæ•°é‡å·²ç»å¾ˆå°‘ï¼Œæ— éœ€è¿‡æ»¤
            return ocr_summary_text
        
        # ç®€å•å¯å‘å¼ï¼šä¿ç•™å‰top_kä¸ªåŒºåŸŸï¼ˆå‡è®¾å·²æŒ‰ç›¸å…³æ€§æ’åºï¼‰
        # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›åŒºåŸŸåº”è¯¥å·²ç»é€šè¿‡ layout_relevance_selector æ’åºè¿‡
        cleaned_regions = regions[:top_k]
        
        # é‡æ–°æ„å»ºæ¸…æ´—åçš„æ–‡æœ¬
        cleaned_text = f"Top {top_k} Most Relevant Regions:\n"
        for i, region_text in enumerate(cleaned_regions, 1):
            cleaned_text += f"Region {i}: {region_text.strip()}\n"
        
        return cleaned_text
    
    def refine_with_ocr_context(self, 
                                 image_path: str, 
                                 user_query: str,
                                 ocr_summary_text: str) -> str:
        """
        åŸºäºOCRç»“æœæç¤ºï¼Œè®©Qwen2-VLé‡æ–°è¿›è¡ŒOCRè¯†åˆ«ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            image_path: åŸå§‹å›¾ç‰‡è·¯å¾„
            user_query: ç”¨æˆ·çš„æŸ¥è¯¢/ä»»åŠ¡
            ocr_summary_text: OCRè¯†åˆ«ç»“æœçš„æ‘˜è¦æ–‡æœ¬
            
        Returns:
            Qwen2-VLåŸºäºOCRæç¤ºå’Œå›¾ç‰‡ç»™å‡ºçš„è¯†åˆ«ç»“æœ
        """
        self.load_model()
        
        # æ­¥éª¤1: æ¸…æ´—OCRä¸Šä¸‹æ–‡ï¼ˆå‡å°‘å™ªå£°ï¼‰
        cleaned_ocr_text = self._clean_ocr_context(ocr_summary_text, top_k=3)
        
        # æ­¥éª¤2: æ„å»ºä¼˜åŒ–çš„æç¤ºè¯
        prompt_text = self._build_refine_prompt(user_query, cleaned_ocr_text)
        
        # æ­¥éª¤3: å‡†å¤‡å›¾ç‰‡å’Œå¯¹è¯
        conversation = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image_path,
                    },
                    {"type": "text", "text": prompt_text},
                ],
            }
        ]
        
        # æ­¥éª¤4: åº”ç”¨èŠå¤©æ¨¡æ¿
        text_prompt = self.processor.apply_chat_template(
            conversation, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # æ­¥éª¤5: å¤„ç†å›¾ç‰‡å’Œæ–‡æœ¬
        image_inputs, video_inputs = process_vision_info(conversation)
        inputs = self.processor(
            text=[text_prompt],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device)
        
        # æ­¥éª¤6: ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs, 
                max_new_tokens=512,
                temperature=0.1,
                top_p=0.9,
                top_k=50
            )
        
        # æ­¥éª¤7: è§£ç è¾“å‡º
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        response = self.processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        return response[0].strip()
    
    def direct_inference(self, image_path: str, user_query: str) -> str:
        """
        ç›´æ¥æ¨ç†æ¨¡å¼ï¼ˆä¸ä½¿ç”¨OCRä¸Šä¸‹æ–‡ï¼‰
        é€‚ç”¨äºç®€å•è¯†åˆ«ä»»åŠ¡
        
        Args:
            image_path: åŸå§‹å›¾ç‰‡è·¯å¾„
            user_query: ç”¨æˆ·çš„æŸ¥è¯¢/ä»»åŠ¡
            
        Returns:
            Qwen2-VL ç›´æ¥åŸºäºå›¾ç‰‡ç»™å‡ºçš„è¯†åˆ«ç»“æœ
        """
        self.load_model()
        
        print(f"[QwenRefinerOptimized] ç›´æ¥æ¨ç†æ¨¡å¼ï¼ˆæ—  OCR è¾…åŠ©ï¼‰...")
        
        # å‡†å¤‡å¯¹è¯
        conversation = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image_path,
                    },
                    {"type": "text", "text": user_query},
                ],
            }
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text_prompt = self.processor.apply_chat_template(
            conversation, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # å¤„ç†å›¾ç‰‡å’Œæ–‡æœ¬
        image_inputs, video_inputs = process_vision_info(conversation)
        inputs = self.processor(
            text=[text_prompt],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device)
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs, 
                max_new_tokens=512,
                temperature=0.1,
                top_p=0.9,
                top_k=50
            )
        
        # è§£ç è¾“å‡º
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        response = self.processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        return response[0].strip()
    
    def _build_refine_prompt(self, user_query: str, ocr_summary_text: str) -> str:
        """
        æ„å»ºç»™Qwen2-VLçš„ä¼˜åŒ–æç¤ºè¯
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. è¯†åˆ«queryç±»å‹ï¼ˆnumber/title/nameç­‰ï¼‰
        2. ç”Ÿæˆé’ˆå¯¹æ€§çš„task hint
        3. å¼ºåŒ–STEP-BY-STEPæŒ‡ä»¤
        4. æ·»åŠ CRITICAL RULESå¼ºè°ƒè§†è§‰éªŒè¯
        5. å¼±åŒ–OCRä½œä¸ºç­”æ¡ˆæ¥æº
        """
        # åˆ†æqueryç±»å‹
        query_lower = user_query.lower()
        task_hint = ""
        
        if 'number' in query_lower or 'digit' in query_lower or 'æ•°å­—' in user_query:
            task_hint = "**TASK**: Extract the NUMBER from the image. VERIFY visually - don't be distracted by other numbers in irrelevant areas."
        elif 'title' in query_lower or 'æ ‡é¢˜' in user_query:
            task_hint = "**TASK**: Find the TITLE in the image. Usually it's the most prominent text at the top. VERIFY by looking at the image."
        elif 'name' in query_lower or 'åå­—' in user_query or 'author' in query_lower:
            task_hint = "**TASK**: Identify the NAME/AUTHOR in the image. VERIFY by looking at the image, not just OCR text."
        else:
            task_hint = "**TASK**: Answer the question by LOOKING AT THE IMAGE carefully."
        
        # æ„å»ºä¼˜åŒ–åçš„prompt
        prompt = f"""You are a visual question answering assistant with strong visual understanding capabilities.

{task_hint}

**Question**: {user_query}

**STEP-BY-STEP Instructions**:
1. **LOOK AT THE IMAGE FIRST** - Understand the visual content and layout
2. **Identify the target** - Find where the answer is located in the image
3. **Verify visually** - Read the text directly from the image
4. **Cross-check** - Use OCR hints below ONLY to confirm location, NOT as the answer source
5. **Focus on relevance** - IGNORE all text from irrelevant regions (wrong page, different section, etc.)
6. **Extract answer** - Provide the exact text/number you see in the target location
7. **Be concise** - Give the direct answer without extra explanation

**CRITICAL RULES**:
1. The IMAGE is your PRIMARY source - trust what you SEE
2. Use OCR text ONLY as location hints, NOT as the answer
3. IGNORE all OCR text from irrelevant regions and any other text not related to the question
4. If the question asks for a NUMBER, respond with ONLY that number
5. If OCR text contradicts what you see in the image, TRUST THE IMAGE

**Top 3 Most Relevant OCR Regions** (use ONLY as location hints):
{ocr_summary_text}

**Remember**: LOOK AT THE IMAGE first, then verify with OCR hints. Don't let irrelevant OCR text distract you."""

        return prompt
    
    def refine_from_summary_file(self, summary_json_path: str) -> Dict[str, Any]:
        """
        ä»æ‘˜è¦JSONæ–‡ä»¶è¯»å–ä¿¡æ¯ï¼Œå¹¶è°ƒç”¨Qwen2-VLè¿›è¡Œrefinementï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            summary_json_path: æ‘˜è¦JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«refined_responseå’Œå…¶ä»–å…ƒä¿¡æ¯çš„å­—å…¸
        """
        # è¯»å–æ‘˜è¦JSON
        with open(summary_json_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
        
        # æå–å…³é”®ä¿¡æ¯
        image_path = summary_data.get('image_path')
        user_query = summary_data.get('user_query')
        
        # æå–OCRæ‘˜è¦æ–‡æœ¬
        summary_sections = summary_data.get('summary', {})
        
        # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µå
        ocr_summary_text = (
            summary_sections.get('relevant_regions') or 
            summary_sections.get('selected_layouts') or
            summary_sections.get('layout_summary') or
            summary_sections.get('ocr_summary') or
            ""
        )
        
        # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½¬ä¸ºæ–‡æœ¬
        if isinstance(ocr_summary_text, dict):
            # ä»å­—å…¸æå–æ–‡æœ¬
            if 'regions' in ocr_summary_text:
                regions = ocr_summary_text['regions']
                if isinstance(regions, list):
                    ocr_summary_text = "\n".join([
                        f"Region {i+1}: {r.get('text', r.get('content', ''))}" 
                        for i, r in enumerate(regions)
                    ])
        elif isinstance(ocr_summary_text, list):
            # ä»åˆ—è¡¨æå–æ–‡æœ¬
            ocr_summary_text = "\n".join([
                f"Region {i+1}: {item.get('text', item.get('content', str(item)))}" 
                for i, item in enumerate(ocr_summary_text)
            ])
        
        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°OCRæ–‡æœ¬ï¼Œå°è¯•ä»merged_blocksè·å–
        if not ocr_summary_text and 'merged_blocks' in summary_data:
            blocks = summary_data['merged_blocks']
            if isinstance(blocks, list) and len(blocks) > 0:
                ocr_summary_text = "\n".join([
                    f"Block {i+1}: {block.get('text', '')}" 
                    for i, block in enumerate(blocks[:5])  # æœ€å¤šå–å‰5ä¸ª
                ])
        
        # è°ƒç”¨ä¼˜åŒ–ç‰ˆçš„refinement
        if ocr_summary_text:
            print(f"[QwenRefinerOptimized] ä½¿ç”¨ OCR ä¸Šä¸‹æ–‡...")
            print(f"[QwenRefinerOptimized]   å›¾ç‰‡: {Path(image_path).name}")
            print(f"[QwenRefinerOptimized]   Query: '{user_query}'")
            print(f"[QwenRefinerOptimized]   OCRæ–‡æœ¬é•¿åº¦: {len(ocr_summary_text)} å­—ç¬¦")
            
            refined_response = self.refine_with_ocr_context(
                image_path=image_path,
                user_query=user_query,
                ocr_summary_text=ocr_summary_text
            )
        else:
            # é™çº§ä¸ºç›´æ¥æ¨ç†
            print(f"[QwenRefinerOptimized] âš ï¸  æœªæ‰¾åˆ°OCRæ‘˜è¦ï¼Œä½¿ç”¨ç›´æ¥æ¨ç†...")
            refined_response = self.direct_inference(image_path, user_query)
        
        # è¿”å›ç»“æœ
        result = {
            'refined_response': refined_response,
            'image_path': image_path,
            'user_query': user_query,
            'ocr_summary_used': bool(ocr_summary_text),
            'summary_file': summary_json_path
        }
        
        return result


def test_refiner():
    """æµ‹è¯•Qwen Refiner Optimized"""
    print("=" * 80)
    print("æµ‹è¯• Qwen2-VL Refiner (Optimized Version)")
    print("=" * 80)
    
    # ç¤ºä¾‹ï¼šä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„æ‘˜è¦æ–‡ä»¶
    summary_file = "case2_output/example_task/result_summary.json"
    
    if not Path(summary_file).exists():
        print(f"\nâŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {summary_file}")
        print("è¯·å…ˆè¿è¡Œ pipeline.py ç”Ÿæˆæµ‹è¯•æ•°æ®")
        return
    
    # åˆ›å»º refinerï¼ˆä¸ä½¿ç”¨ ctxï¼Œç‹¬ç«‹æµ‹è¯•ï¼‰
    refiner = QwenRefinerOptimized(model_path="models/Qwen2-VL-2B-Instruct")
    
    # æ‰§è¡Œrefinement
    print("\næ­£åœ¨åŸºäºOCRç»“æœè°ƒç”¨Qwen2-VL (Optimized)...")
    result = refiner.refine_from_summary_file(summary_file)
    
    # ä¿å­˜ç»“æœ
    output_file = "case2_output/example_task/refined_answer_optimized.json"
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 80)
    print("Qwen2-VL Refinement ç»“æœ (Optimized)")
    print("=" * 80)
    print(f"\nğŸ“· å›¾åƒ: {result['image_path']}")
    print(f"â“ ç”¨æˆ·æŸ¥è¯¢: {result['user_query']}")
    print(f"ğŸ“„ ä½¿ç”¨çš„OCRæ‘˜è¦: {result['ocr_summary_used']}")
    print("\n" + "-" * 80)
    print("ğŸ’¡ Qwen2-VL çš„å›ç­” (Optimized):")
    print("-" * 80)
    print(result['refined_response'])
    print("\n" + "=" * 80)
    print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 80)


if __name__ == "__main__":
    test_refiner()
